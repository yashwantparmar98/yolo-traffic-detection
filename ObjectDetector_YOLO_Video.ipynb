{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObjectDetector_YOLO_Video.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nddL3NW31CK9",
        "colab_type": "code",
        "outputId": "d1bc8b0d-1235-4fe5-8e42-d4ad36bfb329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pdhruv93/YOLO-Face-Recognition-DLIB.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'YOLO-Object-Detection'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/23)   \u001b[K\rremote: Counting objects:   8% (2/23)   \u001b[K\rremote: Counting objects:  13% (3/23)   \u001b[K\rremote: Counting objects:  17% (4/23)   \u001b[K\rremote: Counting objects:  21% (5/23)   \u001b[K\rremote: Counting objects:  26% (6/23)   \u001b[K\rremote: Counting objects:  30% (7/23)   \u001b[K\rremote: Counting objects:  34% (8/23)   \u001b[K\rremote: Counting objects:  39% (9/23)   \u001b[K\rremote: Counting objects:  43% (10/23)   \u001b[K\rremote: Counting objects:  47% (11/23)   \u001b[K\rremote: Counting objects:  52% (12/23)   \u001b[K\rremote: Counting objects:  56% (13/23)   \u001b[K\rremote: Counting objects:  60% (14/23)   \u001b[K\rremote: Counting objects:  65% (15/23)   \u001b[K\rremote: Counting objects:  69% (16/23)   \u001b[K\rremote: Counting objects:  73% (17/23)   \u001b[K\rremote: Counting objects:  78% (18/23)   \u001b[K\rremote: Counting objects:  82% (19/23)   \u001b[K\rremote: Counting objects:  86% (20/23)   \u001b[K\rremote: Counting objects:  91% (21/23)   \u001b[K\rremote: Counting objects:  95% (22/23)   \u001b[K\rremote: Counting objects: 100% (23/23)   \u001b[K\rremote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 23 (delta 4), reused 22 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (23/23), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1tP_xUspJaK6",
        "colab_type": "code",
        "outputId": "2cc74a46-c3b3-4ec3-82f2-0430f0d3ad47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd YOLO-Object-Detection//yolo-object-detection"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/YOLO-Object-Detection/yolo-object-detection/videos/YOLO-Object-Detection/yolo-object-detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XS917czhWi7W",
        "colab_type": "code",
        "outputId": "21ecf25e-14d5-46c9-d336-2f852c0ddfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget \"https://pjreddie.com/media/files/yolov3.weights\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-25 21:46:33--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  42.9MB/s    in 5.4s    \n",
            "\n",
            "2019-02-25 21:46:39 (43.9 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3qRaZzSDKZe1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OX9Voz5VKaHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videoInPath=\"videos//car_chase_01.mp4\"\n",
        "videoOutPath=\"videos//car_chase_01_yolo.avi\"\n",
        "yoloPath=\"yolo-coco\"\n",
        "# command line arguments in dict form\n",
        "#video-- path to input video\n",
        "#yolo --base path to YOLO directory\n",
        "#confidence --minimum probability to filter weak detections\n",
        "#threshold --threshold when applyong non-maxima suppression\n",
        "args={'videoIn': videoInPath , 'videoOut': videoOutPath , 'yolo': yoloPath , 'confidence': 0.5, 'threshold': 0.3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0MSlvBYKfPn",
        "colab_type": "code",
        "outputId": "5bb21248-a066-4816-c6d0-1861652c0c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load the COCO class labels our YOLO model was trained on\n",
        "LABELS = open(yoloPath+\"//coco.names\").read().strip().split(\"\\n\")\n",
        "print(\"Toal classes {0}\".format(len(LABELS)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toal classes 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjfwpEsIKjsD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize a list of colors to represent each possible class label\n",
        "np.random.seed(42)\n",
        "#create random list of int type numbers from range 0-255. Size = len(LABELS), 3... 3 is for RGB\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),dtype=\"uint8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdEW-O_kKkq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = \"yolov3.weights\"\n",
        "configPath = yoloPath+\"//yolov3.cfg\"\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u301D1WbKmu7",
        "colab_type": "code",
        "outputId": "4da852a8-77b3-4f31-a5fb-a15c8b6c01c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Generally in a sequential CNN network there will be only one output layer at the end. \n",
        "#In the YOLO v3 architecture we are using there are multiple output layers giving out predictions.\n",
        "ln_all = net.getLayerNames()\n",
        "#print(ln)\n",
        "\n",
        "ln=[]\n",
        "# determine only the *output* layer names that we need from YOLO\n",
        "for i in ln_all:\n",
        "    if \"yolo\" in i:\n",
        "        ln.append(i)\n",
        "        \n",
        "print(ln)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yolo_82', 'yolo_94', 'yolo_106']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0vSiFTfWKqg4",
        "colab_type": "code",
        "outputId": "e9770a55-2b08-458e-a28e-3239a659e3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6001
        }
      },
      "cell_type": "code",
      "source": [
        "#Open a file pointer to the video file\n",
        "vs = cv2.VideoCapture(args[\"videoIn\"])\n",
        "\n",
        "writer = None #Initialize video writer\n",
        "(W, H) = (None, None) #Initialize frame dimensions\n",
        "\n",
        "# try to determine the total number of frames in the video file\n",
        "try:\n",
        "    total = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(\"[INFO] {} total frames in video\".format(total))\n",
        "except:\n",
        "    # an error occurred while trying to determine the total number of frames in the video file\n",
        "    print(\"[INFO] could not determine # of frames in video\")\n",
        "    print(\"[INFO] no approx. completion time can be provided\")\n",
        "    total = -1\n",
        "\n",
        "currentFrameNumber=1\n",
        "\n",
        "try:\n",
        "    # loop over frames from the video file stream\n",
        "    while True:\n",
        "        # read the next frame from the file\n",
        "        (grabbed, frame) = vs.read()\n",
        "\n",
        "        # if the frame was not grabbed, then we have reached the end of the stream\n",
        "        if not grabbed:\n",
        "            break\n",
        "\n",
        "        #break if current frame count exceeds total count\n",
        "        if(currentFrameNumber > total):\n",
        "            break\n",
        "\n",
        "        #break if q from keyboard is read\n",
        "        if(cv2.waitKey(1) == ord('q')):\n",
        "            break\n",
        "\n",
        "        print(\"Grabbed frame.....{}\".format(currentFrameNumber))\n",
        "\n",
        "        # if the frame dimensions are empty, grab them\n",
        "        if W is None or H is None:\n",
        "            (H, W) = frame.shape[:2]\n",
        "\n",
        "        # construct a blob from the input frame\n",
        "        #why we do : preprocessing images and preparing them for classification via pre-trained deep learning models.\n",
        "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
        "\n",
        "        net.setInput(blob)\n",
        "\n",
        "        #perform a forward pass of the YOLO object detector, giving us our bounding boxes and associated probabilities\n",
        "        layerOutputs = net.forward(ln)\n",
        "\n",
        "        #yolo has now processed our image and got the data. we now just need to fetch that data\n",
        "        # initialize empty lists\n",
        "        boxes = []\n",
        "        confidences = []\n",
        "        classIDs = []\n",
        "\n",
        "        #Let’s begin populating these lists with data from our YOLO layerOutputs\n",
        "        # loop over each of the layer outputs\n",
        "        for output in layerOutputs:\n",
        "            # loop over each of the detections\n",
        "            for detection in output:\n",
        "                # extract the class ID and confidence (i.e., probability) of the current object detection\n",
        "                scores = detection[5:]\n",
        "                classID = np.argmax(scores)\n",
        "                confidence = scores[classID]\n",
        "\n",
        "                # filter out weak predictions by ensuring the detected probability is greater than the minimum probability\n",
        "                if confidence > args[\"confidence\"]:\n",
        "                    # scale the bounding box coordinates back relative to the size of the image\n",
        "                    #YOLO returns the center (x, y)-coordinates of the bounding box followed by the box width and height\n",
        "                    box = detection[0:4] * np.array([W, H, W, H])\n",
        "\n",
        "                    #astype(\"int\") witll convert box values to int values\n",
        "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                    # use the center (x, y)-coordinates to derive the top-left corner of the bounding box\n",
        "                    x = int(centerX - (width / 2))\n",
        "                    y = int(centerY - (height / 2))\n",
        "\n",
        "                    # update our list of bounding box coordinates, confidences and class IDs\n",
        "                    boxes.append([x, y, int(width), int(height)])\n",
        "                    confidences.append(float(confidence))\n",
        "                    classIDs.append(classID)\n",
        "\n",
        "        # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
        "        #All that is required is that we submit our:\n",
        "        #bounding boxes , confidences , confidence threshold and NMS threshold\n",
        "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, args[\"confidence\"],args[\"threshold\"])\n",
        "\n",
        "        #idxs now hold indexes after non maxima suppression\n",
        "\n",
        "\n",
        "        #draw the boxes and class text on the frame\n",
        "        if len(idxs) > 0:\n",
        "            for i in idxs.flatten():\n",
        "                # extract the bounding box coordinates\n",
        "                (x, y) = (boxes[i][0], boxes[i][1]) #x,y--coordinate of top left corner\n",
        "                (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "                #pick the color\n",
        "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
        "\n",
        "                # draw a bounding box rectangle\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)  #2 is the line thickness\n",
        "\n",
        "                #prepare text\n",
        "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "\n",
        "                #put text on image at x, y-5....a bit up then the box\n",
        "                cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, color, 2)\n",
        "\n",
        "        # check if the video writer is None\n",
        "        if writer is None:\n",
        "            # initialize our video writer\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\n",
        "            writer = cv2.VideoWriter(args[\"videoOut\"], fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "        # write the output frame to disk\n",
        "        writer.write(frame)\n",
        "\n",
        "        currentFrameNumber=currentFrameNumber+1\n",
        "\n",
        "except: \n",
        "  pass\n",
        "    \n",
        "# release the file pointers\n",
        "print(\"[INFO] cleaning up...\")\n",
        "\n",
        "if writer is not None:\n",
        "  writer.release()\n",
        "  vs.release()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 350 total frames in video\n",
            "Grabbed frame.....1\n",
            "Grabbed frame.....2\n",
            "Grabbed frame.....3\n",
            "Grabbed frame.....4\n",
            "Grabbed frame.....5\n",
            "Grabbed frame.....6\n",
            "Grabbed frame.....7\n",
            "Grabbed frame.....8\n",
            "Grabbed frame.....9\n",
            "Grabbed frame.....10\n",
            "Grabbed frame.....11\n",
            "Grabbed frame.....12\n",
            "Grabbed frame.....13\n",
            "Grabbed frame.....14\n",
            "Grabbed frame.....15\n",
            "Grabbed frame.....16\n",
            "Grabbed frame.....17\n",
            "Grabbed frame.....18\n",
            "Grabbed frame.....19\n",
            "Grabbed frame.....20\n",
            "Grabbed frame.....21\n",
            "Grabbed frame.....22\n",
            "Grabbed frame.....23\n",
            "Grabbed frame.....24\n",
            "Grabbed frame.....25\n",
            "Grabbed frame.....26\n",
            "Grabbed frame.....27\n",
            "Grabbed frame.....28\n",
            "Grabbed frame.....29\n",
            "Grabbed frame.....30\n",
            "Grabbed frame.....31\n",
            "Grabbed frame.....32\n",
            "Grabbed frame.....33\n",
            "Grabbed frame.....34\n",
            "Grabbed frame.....35\n",
            "Grabbed frame.....36\n",
            "Grabbed frame.....37\n",
            "Grabbed frame.....38\n",
            "Grabbed frame.....39\n",
            "Grabbed frame.....40\n",
            "Grabbed frame.....41\n",
            "Grabbed frame.....42\n",
            "Grabbed frame.....43\n",
            "Grabbed frame.....44\n",
            "Grabbed frame.....45\n",
            "Grabbed frame.....46\n",
            "Grabbed frame.....47\n",
            "Grabbed frame.....48\n",
            "Grabbed frame.....49\n",
            "Grabbed frame.....50\n",
            "Grabbed frame.....51\n",
            "Grabbed frame.....52\n",
            "Grabbed frame.....53\n",
            "Grabbed frame.....54\n",
            "Grabbed frame.....55\n",
            "Grabbed frame.....56\n",
            "Grabbed frame.....57\n",
            "Grabbed frame.....58\n",
            "Grabbed frame.....59\n",
            "Grabbed frame.....60\n",
            "Grabbed frame.....61\n",
            "Grabbed frame.....62\n",
            "Grabbed frame.....63\n",
            "Grabbed frame.....64\n",
            "Grabbed frame.....65\n",
            "Grabbed frame.....66\n",
            "Grabbed frame.....67\n",
            "Grabbed frame.....68\n",
            "Grabbed frame.....69\n",
            "Grabbed frame.....70\n",
            "Grabbed frame.....71\n",
            "Grabbed frame.....72\n",
            "Grabbed frame.....73\n",
            "Grabbed frame.....74\n",
            "Grabbed frame.....75\n",
            "Grabbed frame.....76\n",
            "Grabbed frame.....77\n",
            "Grabbed frame.....78\n",
            "Grabbed frame.....79\n",
            "Grabbed frame.....80\n",
            "Grabbed frame.....81\n",
            "Grabbed frame.....82\n",
            "Grabbed frame.....83\n",
            "Grabbed frame.....84\n",
            "Grabbed frame.....85\n",
            "Grabbed frame.....86\n",
            "Grabbed frame.....87\n",
            "Grabbed frame.....88\n",
            "Grabbed frame.....89\n",
            "Grabbed frame.....90\n",
            "Grabbed frame.....91\n",
            "Grabbed frame.....92\n",
            "Grabbed frame.....93\n",
            "Grabbed frame.....94\n",
            "Grabbed frame.....95\n",
            "Grabbed frame.....96\n",
            "Grabbed frame.....97\n",
            "Grabbed frame.....98\n",
            "Grabbed frame.....99\n",
            "Grabbed frame.....100\n",
            "Grabbed frame.....101\n",
            "Grabbed frame.....102\n",
            "Grabbed frame.....103\n",
            "Grabbed frame.....104\n",
            "Grabbed frame.....105\n",
            "Grabbed frame.....106\n",
            "Grabbed frame.....107\n",
            "Grabbed frame.....108\n",
            "Grabbed frame.....109\n",
            "Grabbed frame.....110\n",
            "Grabbed frame.....111\n",
            "Grabbed frame.....112\n",
            "Grabbed frame.....113\n",
            "Grabbed frame.....114\n",
            "Grabbed frame.....115\n",
            "Grabbed frame.....116\n",
            "Grabbed frame.....117\n",
            "Grabbed frame.....118\n",
            "Grabbed frame.....119\n",
            "Grabbed frame.....120\n",
            "Grabbed frame.....121\n",
            "Grabbed frame.....122\n",
            "Grabbed frame.....123\n",
            "Grabbed frame.....124\n",
            "Grabbed frame.....125\n",
            "Grabbed frame.....126\n",
            "Grabbed frame.....127\n",
            "Grabbed frame.....128\n",
            "Grabbed frame.....129\n",
            "Grabbed frame.....130\n",
            "Grabbed frame.....131\n",
            "Grabbed frame.....132\n",
            "Grabbed frame.....133\n",
            "Grabbed frame.....134\n",
            "Grabbed frame.....135\n",
            "Grabbed frame.....136\n",
            "Grabbed frame.....137\n",
            "Grabbed frame.....138\n",
            "Grabbed frame.....139\n",
            "Grabbed frame.....140\n",
            "Grabbed frame.....141\n",
            "Grabbed frame.....142\n",
            "Grabbed frame.....143\n",
            "Grabbed frame.....144\n",
            "Grabbed frame.....145\n",
            "Grabbed frame.....146\n",
            "Grabbed frame.....147\n",
            "Grabbed frame.....148\n",
            "Grabbed frame.....149\n",
            "Grabbed frame.....150\n",
            "Grabbed frame.....151\n",
            "Grabbed frame.....152\n",
            "Grabbed frame.....153\n",
            "Grabbed frame.....154\n",
            "Grabbed frame.....155\n",
            "Grabbed frame.....156\n",
            "Grabbed frame.....157\n",
            "Grabbed frame.....158\n",
            "Grabbed frame.....159\n",
            "Grabbed frame.....160\n",
            "Grabbed frame.....161\n",
            "Grabbed frame.....162\n",
            "Grabbed frame.....163\n",
            "Grabbed frame.....164\n",
            "Grabbed frame.....165\n",
            "Grabbed frame.....166\n",
            "Grabbed frame.....167\n",
            "Grabbed frame.....168\n",
            "Grabbed frame.....169\n",
            "Grabbed frame.....170\n",
            "Grabbed frame.....171\n",
            "Grabbed frame.....172\n",
            "Grabbed frame.....173\n",
            "Grabbed frame.....174\n",
            "Grabbed frame.....175\n",
            "Grabbed frame.....176\n",
            "Grabbed frame.....177\n",
            "Grabbed frame.....178\n",
            "Grabbed frame.....179\n",
            "Grabbed frame.....180\n",
            "Grabbed frame.....181\n",
            "Grabbed frame.....182\n",
            "Grabbed frame.....183\n",
            "Grabbed frame.....184\n",
            "Grabbed frame.....185\n",
            "Grabbed frame.....186\n",
            "Grabbed frame.....187\n",
            "Grabbed frame.....188\n",
            "Grabbed frame.....189\n",
            "Grabbed frame.....190\n",
            "Grabbed frame.....191\n",
            "Grabbed frame.....192\n",
            "Grabbed frame.....193\n",
            "Grabbed frame.....194\n",
            "Grabbed frame.....195\n",
            "Grabbed frame.....196\n",
            "Grabbed frame.....197\n",
            "Grabbed frame.....198\n",
            "Grabbed frame.....199\n",
            "Grabbed frame.....200\n",
            "Grabbed frame.....201\n",
            "Grabbed frame.....202\n",
            "Grabbed frame.....203\n",
            "Grabbed frame.....204\n",
            "Grabbed frame.....205\n",
            "Grabbed frame.....206\n",
            "Grabbed frame.....207\n",
            "Grabbed frame.....208\n",
            "Grabbed frame.....209\n",
            "Grabbed frame.....210\n",
            "Grabbed frame.....211\n",
            "Grabbed frame.....212\n",
            "Grabbed frame.....213\n",
            "Grabbed frame.....214\n",
            "Grabbed frame.....215\n",
            "Grabbed frame.....216\n",
            "Grabbed frame.....217\n",
            "Grabbed frame.....218\n",
            "Grabbed frame.....219\n",
            "Grabbed frame.....220\n",
            "Grabbed frame.....221\n",
            "Grabbed frame.....222\n",
            "Grabbed frame.....223\n",
            "Grabbed frame.....224\n",
            "Grabbed frame.....225\n",
            "Grabbed frame.....226\n",
            "Grabbed frame.....227\n",
            "Grabbed frame.....228\n",
            "Grabbed frame.....229\n",
            "Grabbed frame.....230\n",
            "Grabbed frame.....231\n",
            "Grabbed frame.....232\n",
            "Grabbed frame.....233\n",
            "Grabbed frame.....234\n",
            "Grabbed frame.....235\n",
            "Grabbed frame.....236\n",
            "Grabbed frame.....237\n",
            "Grabbed frame.....238\n",
            "Grabbed frame.....239\n",
            "Grabbed frame.....240\n",
            "Grabbed frame.....241\n",
            "Grabbed frame.....242\n",
            "Grabbed frame.....243\n",
            "Grabbed frame.....244\n",
            "Grabbed frame.....245\n",
            "Grabbed frame.....246\n",
            "Grabbed frame.....247\n",
            "Grabbed frame.....248\n",
            "Grabbed frame.....249\n",
            "Grabbed frame.....250\n",
            "Grabbed frame.....251\n",
            "Grabbed frame.....252\n",
            "Grabbed frame.....253\n",
            "Grabbed frame.....254\n",
            "Grabbed frame.....255\n",
            "Grabbed frame.....256\n",
            "Grabbed frame.....257\n",
            "Grabbed frame.....258\n",
            "Grabbed frame.....259\n",
            "Grabbed frame.....260\n",
            "Grabbed frame.....261\n",
            "Grabbed frame.....262\n",
            "Grabbed frame.....263\n",
            "Grabbed frame.....264\n",
            "Grabbed frame.....265\n",
            "Grabbed frame.....266\n",
            "Grabbed frame.....267\n",
            "Grabbed frame.....268\n",
            "Grabbed frame.....269\n",
            "Grabbed frame.....270\n",
            "Grabbed frame.....271\n",
            "Grabbed frame.....272\n",
            "Grabbed frame.....273\n",
            "Grabbed frame.....274\n",
            "Grabbed frame.....275\n",
            "Grabbed frame.....276\n",
            "Grabbed frame.....277\n",
            "Grabbed frame.....278\n",
            "Grabbed frame.....279\n",
            "Grabbed frame.....280\n",
            "Grabbed frame.....281\n",
            "Grabbed frame.....282\n",
            "Grabbed frame.....283\n",
            "Grabbed frame.....284\n",
            "Grabbed frame.....285\n",
            "Grabbed frame.....286\n",
            "Grabbed frame.....287\n",
            "Grabbed frame.....288\n",
            "Grabbed frame.....289\n",
            "Grabbed frame.....290\n",
            "Grabbed frame.....291\n",
            "Grabbed frame.....292\n",
            "Grabbed frame.....293\n",
            "Grabbed frame.....294\n",
            "Grabbed frame.....295\n",
            "Grabbed frame.....296\n",
            "Grabbed frame.....297\n",
            "Grabbed frame.....298\n",
            "Grabbed frame.....299\n",
            "Grabbed frame.....300\n",
            "Grabbed frame.....301\n",
            "Grabbed frame.....302\n",
            "Grabbed frame.....303\n",
            "Grabbed frame.....304\n",
            "Grabbed frame.....305\n",
            "Grabbed frame.....306\n",
            "Grabbed frame.....307\n",
            "Grabbed frame.....308\n",
            "Grabbed frame.....309\n",
            "Grabbed frame.....310\n",
            "Grabbed frame.....311\n",
            "Grabbed frame.....312\n",
            "Grabbed frame.....313\n",
            "Grabbed frame.....314\n",
            "Grabbed frame.....315\n",
            "Grabbed frame.....316\n",
            "Grabbed frame.....317\n",
            "Grabbed frame.....318\n",
            "Grabbed frame.....319\n",
            "Grabbed frame.....320\n",
            "Grabbed frame.....321\n",
            "Grabbed frame.....322\n",
            "Grabbed frame.....323\n",
            "Grabbed frame.....324\n",
            "Grabbed frame.....325\n",
            "Grabbed frame.....326\n",
            "Grabbed frame.....327\n",
            "Grabbed frame.....328\n",
            "Grabbed frame.....329\n",
            "Grabbed frame.....330\n",
            "Grabbed frame.....331\n",
            "Grabbed frame.....332\n",
            "Grabbed frame.....333\n",
            "Grabbed frame.....334\n",
            "Grabbed frame.....335\n",
            "Grabbed frame.....336\n",
            "Grabbed frame.....337\n",
            "Grabbed frame.....338\n",
            "Grabbed frame.....339\n",
            "Grabbed frame.....340\n",
            "Grabbed frame.....341\n",
            "Grabbed frame.....342\n",
            "Grabbed frame.....343\n",
            "Grabbed frame.....344\n",
            "Grabbed frame.....345\n",
            "Grabbed frame.....346\n",
            "Grabbed frame.....347\n",
            "Grabbed frame.....348\n",
            "Grabbed frame.....349\n",
            "Grabbed frame.....350\n",
            "[INFO] cleaning up...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dPZJz24znZVp",
        "colab_type": "code",
        "outputId": "c8e52cab-b66a-4602-d0a1-fdada504519f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#code for saving the output video\n",
        "%cd videos\n",
        "from google.colab import files\n",
        "files.download('car_chase_01_yolo.avi')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/YOLO-Object-Detection/yolo-object-detection/videos/YOLO-Object-Detection/yolo-object-detection/videos\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}